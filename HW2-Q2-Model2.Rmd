---
title: "HW2-Q2-Model2"
date: "August 16, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

####Model 2
---

```{r}
library(tm)

## plain text documents in English.
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), id=fname, language='en')
  }
```



```{r}
idf.weight<-function(x){
  doc.freq<-colSums(x>0)
  doc.freq[doc.freq==0]<-1
  w<-log(nrow(x)/doc.freq)
  return(scale.cols(x,w))
}
scale.cols<-function(x,s){
  return(t(apply(x,1,function(x){x*s})))
}

my_cosine = function(v1,v2){
  result=NULL
  for (i in (1:50)){
    result[i]=sum(v1%*%v2[,i])/{sqrt(sum(v1^2))*sqrt(sum(v2[,i]^2))}
  }
  return(result)
}
```

```{r}
library(tm)

## first get all author directories and then the corpus from each author
author_dirs = Sys.glob('data/ReutersC50/C50train/*')

file_list = NULL
labels = NULL

for(author in author_dirs) {
  author_name = substring(author, first=21)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

my_corpus = Corpus(VectorSource(all_docs))
names(my_corpus) = file_list

# Preprocessing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
DTM = removeSparseTerms(DTM, 0.99)
DTM
X=as.matrix(DTM)
row.names(X)=labels
DTM_TF=X/rowSums(X)
DTM_TFIDF=idf.weight(DTM_TF)
```

```{r}
author_dirs = Sys.glob('data/ReutersC50/C50test/*')

file_list = NULL
labels_test = NULL
author_list = NULL

for(author in author_dirs) {
  author_name = substring(author, first=20)
  author_list = append(author_list,author_name)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list

# Preprocessing
test_corpus = tm_map(test_corpus, content_transformer(tolower)) # make everything lowercase
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers)) # remove numbers
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation)) # remove punctuation
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM_test = DocumentTermMatrix(test_corpus)
DTM_test=removeSparseTerms(DTM_test,0.99)
DTM_test

b = DTM_test[,c(intersect(colnames(DTM_TFIDF),colnames(DTM_test)))]

X_test = as.matrix(b)

row.names(X_test)=labels_test
DTM_TF_test=X_test/rowSums(X_test)
DTM_TFIDF_test=idf.weight(DTM_TF_test)

query_vec=t(DTM_TFIDF[,intersect(colnames(X_test),colnames(DTM_TFIDF))])
colnames(query_vec)=labels
query_vec=t(rowsum(t(query_vec),group=rownames(t(query_vec))))
```

```{r}
count=0
predict<-list()
for(i in (1:2500)){
  angle=my_cosine(DTM_TF_test[i,],query_vec)
  predict[[i]]=labels[which.max(angle)]
  k=ceiling(i/50)
  if (predict[[i]]==labels[k]){
    count = count+1
  }
}

predict=do.call(rbind,predict)
count
row.names(predict)=labels_test
```


