---
title: "HW2_Graves_He_Yu_Zhu"
author: "Christina Graves, Mengnan He, Liz Zongyue Yu, Alice Yiqing Zhu"
date: "August 15, 2016"
output: pdf_document
---

##Question 1: Flights at ABIA - What is the best time of year to fly to minimize delays?

```{r, echo=FALSE}
library(ggplot2)
ABIA=read.csv("ABIA.csv")

#Get rid of rows missing/NA values in arrival delay and departure delay columns
ABIA_delay=ABIA[!is.na(ABIA$ArrDelay),]
ABIA_delay=ABIA_delay[!is.na(ABIA_delay$DepDelay),]


#add a new column to add the arrival delay and departure delay
ABIA_delay$DelayTotal<-ABIA_delay$ArrDelay+ABIA_delay$DepDelay

#assign names to Months
ABIA_delay$Month<-factor(ABIA_delay$Month,levels=c(1,2,3,4,5,6,7,8,9,10,11,12),labels=c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))

#plot average flight delay by month
ggplot(ABIA_delay)+geom_bar(aes(Month,DelayTotal,fill=Month),position="dodge",stat="summary",fun.y="mean")+ggtitle("Average Time of Flight Delay by Month")+labs(y="Average Total Time of Arrival/Departure Delay (Minutes)")+theme(legend.position='none')+theme(plot.title = element_text(size=18,face="bold"))

```

**The above figure illustrates that the best time of year to travel in order to minimize arrival and departure flight delays would be during the fall months of September to November. The months in which travelers typically experience longer arrival and departure flight delays are December, March and June. Intuitively, this makes sense because December includes travel for the holidays, March includes travel for Spring Break, and June is the first month of summer vacation. These time periods typically see a influx of travellers, which could be a factor contributing to an increase in flight delays. **

Note: These averages include only the departure and arrival delay times from the dataset due to several reasons. Because the other delay variables (Weather, Security, etc. ) had a very large number of missing values and had very small variances in their delay times in comparison to the arrival and departure delay times, they were excluded from the delay computations in order to use as much of the dataset as possible. For example, the Security Delay variable had 79,513 NA values (80% of the total dataset) and a median of 0 minutes and mean delay time of .07 minutes (about 5 seconds), so including it would not significantly affect delay time per month. 

When all of the delay variables are included with the exception of SecurityDelay, the months with the lowest average duration of delay time remained the same. The months with the highest delay times were December and the summer months of June, July and August. However, we believe this data is significantly skewed and misleading due to extreme outliers in some of the security variables as the month with the lowest average has an average delay time of about two hours, which does not seem reasonable. Additionally, by including all of these variables, we lose about 80% of the dataset due to missing values. Therefore, the original model above appeared to be the best figure in illustrating the best months for minimizing delay time. 

**This figure includes the following variables to calculate Average Length of Delay: Arrival Delay, Departure Delay, Carrier Delay, Weather Delay, NAS Delay and Late Aircraft Delay**

```{r,echo=FALSE}
#include all security variables in calculation of total delay time, except SecurityDelay
ABIA_delay$newTotal=ABIA_delay$ArrDelay+ABIA_delay$DepDelay+ABIA_delay$LateAircraftDelay+ABIA_delay$WeatherDelay+ABIA_delay$NASDelay+ABIA_delay$CarrierDelay
#plot delay time per month
ggplot(ABIA_delay)+geom_bar(aes(Month, newTotal,fill=Month),position="dodge",stat="summary",fun.y="mean")+ggtitle("Average Time of Delay by Month")+labs(y="Average Length of Delay (Minutes)")
```

####What is the best day of the week to travel to minimize delays?

```{r,echo=FALSE}
ABIA_delay$DayOfWeek<-factor(ABIA_delay$DayOfWeek,levels=c(1,2,3,4,5,6,7),labels=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
ggplot(ABIA_delay)+geom_bar(aes(DayOfWeek,DelayTotal,fill=DayOfWeek),position="dodge",stat="summary",fun.y="mean")+ggtitle("Average Time of Flight Delay by Weekday")+labs(y="Average Total Time of Arrival and Departure Delays (Minutes)",x="Day of Week")+theme(legend.position='none')
```

**In going a bit further, we also examined the best time of the week to travel. According to the figure above, Wednesday and Saturday have the lowest average delay times, while Friday is the worst day to travel to minimize delay time.**



##Question2: Text Classification 

####Model 1: Naive Bayes

For Reuter text, we need to make use of the tm package and the readerPlain function as below: 

```{r load pkgs}
library(tm)

#use the readerPlain function in the Reuter example
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
```

We iterate over all 50 authors and read all texts from each one of them, gathering the corresponding author names as a list called "labels" while reading the texts. 

```{r }
## first get all author directories and then the corpus from each author
author_dirs = Sys.glob('ReutersC50/C50train/*')

file_list = NULL
labels = NULL

for(author in author_dirs) {
	author_name = substring(author, first=21)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels = append(labels, rep(author_name, length(files_to_add)))
}
all_docs = lapply(file_list, readerPlain) 
```

Then we build a corpus based on 2500 text files and pre-process them with tm_map() by removing numbers, punctuation, white spaces and stopwords. 

```{r}
# set reasonable names for each document
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
my_corpus = Corpus(VectorSource(all_docs))
names(my_corpus) = file_list

# Preprocessing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

```

Next we use the corpus above to form a document term matrix and remove some sparse terms. 

```{r}
DTM = DocumentTermMatrix(my_corpus)
DTM = removeSparseTerms(DTM, 0.99)
DTM
```

After getting the DTM, we start to build a train set based on it, grouping smoothed train records by authors.

```{r}
# Now a dense matrix
X_train = as.matrix(DTM)

# Naive Bayes: the training sets for the two authors
smooth_count = 1/nrow(X_train)
w_train = rowsum(X_train + smooth_count,labels)
w_train= w_train/sum(w_train)
w_train = log(w_train)
```

We redo every step for the test set before prediction: 

```{r}
#test
author_dirs = Sys.glob('ReutersC50/C50test/*')

file_list = NULL
labels_test = NULL
author_list = NULL

for(author in author_dirs) {
  author_name = substring(author, first=20)
  author_list = append(author_list,author_name)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list

# Preprocessing
test_corpus = tm_map(test_corpus, content_transformer(tolower)) # make everything lowercase
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers)) # remove numbers
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation)) # remove punctuation
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM_test = DocumentTermMatrix(test_corpus,list(dictionary=colnames(DTM)))
DTM_test

X_test = as.matrix(DTM_test)
```

As for prediction, we get maximum Naive Bayes log probabilities for each test row and use the author from the maximum probability as the predicted result. 


```{r}
predict = NULL
for (i in 1:nrow(X_test)) {
  max = -(Inf)
  author = NULL
  for (j in 1:nrow(w_train)) {
    result = sum(w_train[j,]*X_test[i,])
    if(result > max) {
      max = result
      author = rownames(w_train)[j]
    }
  }
  predict = append(predict, author)
}
```

Then we calculate the accuracy of Naive Bayes prediction by confusion matrix and adding all correct prediction for each author. We also calculate the correct rates.

```{r}
predict_results = table(labels_test,predict)
correct = NULL
for (i in 1:nrow(predict_results)) {
  correct = append(correct, predict_results[i, i])
}

pred_table = data.frame(author_list, correct)
pred_table <- pred_table[order(-correct),] 
pred_table$correct_rate <- pred_table$correct/50
```

From the result, we can see that 1/3 of all authors have really high correct rates (more than 80%), while Scott Hillis and Tan EeLyn only get below 20%. 

```{r}
pred_table
```

Overall, the accuracy for all prediction using Naive Bayes is 60.28%. 

```{r}
sum(pred_table$correct)/nrow(X_test)
```

##Question3: 

```{r setup,include=FALSE}
library(arules)
```


**1.** When runing the Apriori Algorithm for the first time, we look at the rules with support level >10% and at the same time loose the confidence restrictions to 0.0001.

By choosing this set of threshold, our goal is to examine the goods commonly shopped by the majority of customers.


```{r,echo=FALSE}
grocery<- readLines("groceries.txt")
grocery<- strsplit(grocery, ",",fixed = FALSE)
grocery <- lapply(grocery, unique)
#str(grocery)
grotrans <- as(grocery, "transactions")
#head(grotrans)
grorules <- apriori(grotrans, 
                    parameter=list(support=.1, confidence=.0001, maxlen=4))

# Look at the output
inspect(grorules)
```

From the results above, a group of rules with only one item (i.e. an empty LHS) like {} => {canned beer} were created. These rules mean that no matter what other items are involved, the item in the RHS will appear with the probability given by the rule's confidence, which equals the support. 

Grocery shoppers'consumption habits differ from each other largely, hence, there's no significantly dominant support number observed (all below 26%), but we can still detect some common trends. Diary products like whole milk and yogurt, vegetables include root vegetables and other vegetables, beverages include bottled water and soda, as well as rolls/buns ranked the top when compared to other goods. For example,25.5% of people buy whole milk no matter what other items are involved in their basket, which make sense since milk consuption has low elasticity and a lot of people needs it in their diet. 


**2.** When we lower the support threshold to 0.05, other single items that have relatively high support numbers include: meat(beef,pork, sausage), diary products(butter, curd, domestic eggs,margarine,sour cream), staple food (rolls/buns,bread,pastry), beverages(beer,juice,coffee),newspapers,napkins and shopping bags.

```{r,echo=FALSE}
grorules <- apriori(grotrans, 
                    parameter=list(support=.05, confidence=.0001, maxlen=4))

# Look at the output
inspect(subset(grorules, subset=support < .1))
```

Most of the categories listed above (except shopping bags) are life necessities, with low elasticity, thus yield higher support fraction among all the other goods. Shopping bags having high support is understandable too. People tend to forget to bring their own shopping bag often.

By looking at other rules that appeared in pairs from above, we see some relationships between yogurt and whole milk, rolls/buns and whole milk, as well as vegetables and whole milk. Though milk is the hottest item among people's grocery shopping list, people who buy yogurt, rolls/buns or other vegetables are 1.4 times in average more likely to buy whole milk than other shoppers.

**3.** Next, we reset the thresholds, with a lower support level, a higher confidence level and lift level, in order to examine goods appeared less often in people's basket, but have stronger in-basket relationships.

In order to avoid rules that contains only one single item, we use the argument parameter: minlen=2.

After testing for different sets of thresholds, we choose to first present support level between 0.4% and 1%, confidence level above 60% and lift above 3. The results are shown below: 

```{r,echo=FALSE}
grorules <- apriori(grotrans, 
                    parameter=list(support=.004, confidence=.6, maxlen=4, minlen=2))

# Look at the output
inspect(subset(grorules, subset=support < .01& lift > 3))
```

Only 0.5% of customers buy sets of two to three items among root vegetables, oinions, pip fruit, tropical fruit,citrus fruit, whole milk, yogurt and sour creams. Among these people, 60% also buy other vegetables. Compared to others, they are 3.2 more likely to buy other vegetables. This reflects a group of people with healty diet style. They might also be vegetarians.

**4.** Next, we try to continue lowering support level in order to detect more rare but strongly related item sets. After trying for several times, we set the support level to (0.0006,0.001), with confidence level >0.6 and lift >15. This gives us a moderate number of rules to deal with.

A too strict confidence level(ie. close to 1) and a low support level yield too many rules. That's why we loose the confidence level to 0.6 and at the same time increase lift threshold.

```{r,echo=FALSE}
grorules <- apriori(grotrans, 
                    parameter=list(support=.0006, confidence=0.6, maxlen=4, minlen=2))

# Look at the output
inspect(subset(grorules, subset=support < .001 & lift > 15))
```


1) We see 0.07% of people have popcorn and bottled water in their basket. 70% of these people also buy salty snack and they are 18.5 times more likely to buy salty snack than others. Similar relationship was observed between ice cream, pastry, soda and salty snack. These people might be couch potatos that like to spend their leisure time eating popcorns and chips and watching TV.

2) Also, we see very strong relationships between instant food products,hamburger meat and hygiene articles/butter/eggs/shopping bags. These shoppers might be very busy and have no time cooking for themsleves. Even the occurance of shopping bags also makes sense. They might go grocery shopping right after work, thus didn't prepare their own shopping bag.


3) We can also observe a group of sweet lovers, they buy waffles, chocolate marshmallow, margarine and yogurt at the same time. Their lift is very high.

4) Sandwich/burger lovers are detected by a strong relationship between frankfurter, ham, processed cheese,tropical fruit, other vegetables, rolls/buns and white bread. This sounds reasonable since people make burgers or sandwiches using these ingredients. 

5) 0.08% of people buy bottled beer, liquor and soda. 66% of them also buy red/blush wine at the same time. Alcohol lovers are 34.69 times more likely than others to have red/blush wine in their basket. 

6) People who like bakery tend to have baking powder, flour, sugar, margarine,chocolate, butter, curd and long life bakery product in their basket. The lift level and confidence are very high, though support level was low.

7) Dairy and pip fruit lovers are also likely to be dessert lovers. Around 80% of people who buy butter milk, curd, pip fruit or whole milk also have dessert in their basket.


**Conclusion**



People's grocery shopping habits largely depend on their diet and life style, thus differ greatly from each other. By setting different sets of threshold, we can detect some common trends among people. However, there are still a lot of rules that we are unable to talk about due to the length of the report.


